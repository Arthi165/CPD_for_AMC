# -*- coding: utf-8 -*-
"""SP1n15ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y-RUHXTP0Qot8r1My6zoGhmp-r_qjojV
"""

# NN for signal proc. 1, n=15

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input, Add, Activation, Flatten
from tensorflow.keras.callbacks import ReduceLROnPlateau
import matplotlib.pyplot as plt

# Load CSV data
csv1_data = pd.read_csv('H1_glrt_10.csv').values.flatten()
csv2_data = pd.read_csv('H2_glrt_10.csv').values.flatten()

# Data generation function with added noise
def generate_data_cpd(N, csv1_data, csv2_data, T):
    Data = []
    Labels = []
    samples_per_tau = N // (T - 1)
    for tau in range(1, T):
        for _ in range(samples_per_tau):
            index_vec1 = np.random.choice(csv1_data, tau)
            index_vec2 = np.random.choice(csv2_data, T - tau)
            data_point = np.concatenate((index_vec1, index_vec2))
            # Introduce random noise
            noise = np.random.normal(0, 0.1, data_point.shape)
            data_point += noise
            Data.append(data_point)
            Labels.append(tau - 1)  # Subtract 1 to make labels zero-based for sparse_categorical_crossentropy

    X_vec = np.array(Data)
    y_vec = np.array(Labels)

    return X_vec, y_vec

# Residual Block with reduced output shape
def residual_block(x, units):
    shortcut = x
    x = Dense(units // 2, activation='tanh')(x)  # Decreased units and changed activation
    x = BatchNormalization()(x)
    x = Dense(units // 2)(x)
    x = BatchNormalization()(x)

    # Ensure the shapes match
    if shortcut.shape[-1] != units // 2:
        shortcut = Dense(units // 2)(shortcut)

    x = Add()([x, shortcut])
    x = Activation('tanh')(x)  # Changed activation to 'tanh'
    return x

# Define parameters
T = 10
N_test = 500
num_runs = 1
N_train_values = [100, 200, 500]

accuracies = []
Pd_vec = np.zeros((len(N_train_values), num_runs))

for i, N_train in enumerate(N_train_values):
    run_accuracies = []

    for run in range(num_runs):
        # Generate training and test data
        X_train, y_train = generate_data_cpd(N_train, csv1_data, csv2_data, T)
        X_test, y_test = generate_data_cpd(N_test, csv1_data, csv2_data, T)

        input_shape = X_train.shape[1:]

        # Fully Connected Neural Network with Residual Blocks
        inputs = Input(shape=input_shape)
        x = Dense(64, activation='tanh')(inputs)  # Decreased units and changed activation
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)  # Increased dropout

        # Add more residual blocks with fewer units
        x = residual_block(x, 32)  # Decreased units
        x = residual_block(x, 64)  # Decreased units

        # Flatten the output
        x = Flatten()(x)

        x = Dense(32, activation='tanh')(x)  # Decreased units and changed activation
        x = Dropout(0.3)(x)  # Increased dropout

        outputs = Dense(T - 1, activation='softmax')(x)

        model = Model(inputs, outputs)

        # Compile the model with a lower learning rate
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Decreased learning rate
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])

        # Callbacks
        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, min_lr=1e-6)

        # Model training without early stopping
        history = model.fit(X_train, y_train, epochs=50, batch_size=32,
                            callbacks=[reduce_lr], verbose=2)

        # Evaluate the model on test data
        y_pred = model.predict(X_test, verbose=0)

        # Correctly compute accuracy
        predicted_labels = np.argmax(y_pred, axis=1)
        correct_predictions = np.sum(predicted_labels == y_test)
        accuracy = correct_predictions / N_test  # Total number of test samples
        run_accuracies.append(accuracy)
        Pd_vec[i, run] = accuracy

    average_accuracy = np.mean(run_accuracies)
    accuracies.append(average_accuracy)
    print(f'N_train = {N_train}, Average Test accuracy over {num_runs} runs: {average_accuracy:.4f}')

# Plotting the accuracy graph
plt.figure(figsize=(10, 6))
plt.plot(N_train_values, accuracies, marker='o')
plt.title('Average Accuracy vs N_train')
plt.xlabel('N_train')
plt.ylabel('Average Accuracy')
plt.grid(True)
plt.show()