# -*- coding: utf-8 -*-
"""SP2n31ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y-RUHXTP0Qot8r1My6zoGhmp-r_qjojV
"""

# NN for no proc., n=31

import pandas as pd
import tensorflow as tf
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization, Conv1D, Flatten, MaxPooling1D, Add, Activation, Input, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tqdm.auto import tqdm
import matplotlib.pyplot as plt

# Function to generate data for change point detection
def generate_data_cpd(N, csv1_data, csv2_data, T):
    Data = []
    Labels = []
    samples_per_tau = N // (T - 1)  # Distribute N equally across tau values

    for tau in range(1, T):
        for _ in range(samples_per_tau):
            index_vec1 = np.random.choice(len(csv1_data), tau, replace=True)
            index_vec2 = np.random.choice(len(csv2_data), T - tau, replace=True)
            data_point = np.vstack((csv1_data[index_vec1], csv2_data[index_vec2]))
            Data.append(data_point)
            Labels.append(tau)

    X_vec = np.array(Data)
    y_vec = np.array(Labels)

    return X_vec, y_vec

# Residual Block
def res_block(x, filters, kernel_size=3):
    shortcut = Conv1D(filters, kernel_size=1, padding='same')(x)
    shortcut = BatchNormalization()(shortcut)

    x = Conv1D(filters, kernel_size=kernel_size, padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = Conv1D(filters, kernel_size=kernel_size, padding='same')(x)
    x = BatchNormalization()(x)

    x = Add()([x, shortcut])
    x = Activation('relu')(x)
    return x

# Load CSV data
csv1_data = pd.read_csv('csv_H1_no_awgn_SNR_10dB.csv').values
csv2_data = pd.read_csv('csv_H2_no_awgn_SNR_10dB.csv').values

# Define parameters
T = 4
N_test = 500
num_runs = 3
N_train_values = [150000, 200000]

accuracies = []
Pd_vec = np.zeros((len(N_train_values), num_runs))

for i, N_train in enumerate(N_train_values):
    run_accuracies = []

    for run in range(num_runs):
        # Generate training and test data
        X_train, y_train = generate_data_cpd(N_train, csv1_data, csv2_data, T)
        X_test, y_test = generate_data_cpd(N_test, csv1_data, csv2_data, T)

        input_shape = (X_train.shape[1], X_train.shape[2])

        # Convolutional Neural Network with Residual Blocks
        inputs = Input(shape=input_shape)
        x = Conv1D(64, kernel_size=3, activation='relu')(inputs)
        x = BatchNormalization()(x)

        if x.shape[1] > 1:
            x = MaxPooling1D(pool_size=2)(x)

        x = res_block(x, 64)

        if x.shape[1] > 1:
            x = MaxPooling1D(pool_size=2)(x)

        x = res_block(x, 128)

        if x.shape[1] > 1:
            x = MaxPooling1D(pool_size=2)(x)

        x = Flatten()(x)

        # Adding Dropout for regularization
        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)
        x = BatchNormalization()(x)
        x = Dropout(0.5)(x)  # Dropout layer
        x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)
        x = BatchNormalization()(x)
        x = Dropout(0.5)(x)  # Dropout layer

        x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)
        x = Dropout(0.5)(x)  # Dropout layer

        outputs = Dense(T, activation='softmax')(x)

        model = Model(inputs, outputs)

        # Lowering the learning rate
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Lower learning rate
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])

        # Callbacks
        early_stopping = EarlyStopping(monitor='loss', patience=50, restore_best_weights=True)
        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=20, min_lr=1e-6)  # Learning rate scheduler

        history = model.fit(X_train, y_train, epochs=30, batch_size=64,
                            callbacks=[early_stopping, reduce_lr], verbose=2)

        # Evaluate the model on test data
        y_pred = model.predict(X_test, verbose=0)
        correct_predictions = np.sum(np.argmax(y_pred, axis=1) == y_test)
        accuracy = correct_predictions / len(y_test)
        run_accuracies.append(accuracy)
        Pd_vec[i, run] = accuracy

        print(f'N_train = {N_train}, Run {run + 1} - Test accuracy: {accuracy}')

    average_accuracy = np.mean(run_accuracies)
    accuracies.append(average_accuracy)
    print(f'N_train = {N_train}, Average Test accuracy over {num_runs} runs: {average_accuracy}')

# Plotting the graph
plt.figure(figsize=(10, 6))
plt.plot(N_train_values, accuracies, marker='o')
plt.title('Average Accuracy vs N_train')
plt.xlabel('N_train')
plt.ylabel('Average Accuracy')
plt.grid(True)
plt.show()