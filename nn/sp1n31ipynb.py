# -*- coding: utf-8 -*-
"""SP2n31ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y-RUHXTP0Qot8r1My6zoGhmp-r_qjojV
"""

# NN for signal proc. 1, n=31

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input, Conv1D, LSTM, GlobalAveragePooling1D
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from collections import Counter

# Load CSV data
csv1_file = 'H1_glrt_10.csv'
csv2_file = 'H2_glrt_10.csv'

# Load CSV data
csv1_data = pd.read_csv(csv1_file).values.flatten()
csv2_data = pd.read_csv(csv2_file).values.flatten()

# Normalize the data
scaler1 = StandardScaler()
scaler2 = StandardScaler()
csv1_data = scaler1.fit_transform(csv1_data.reshape(-1, 1)).flatten()
csv2_data = scaler2.fit_transform(csv2_data.reshape(-1, 1)).flatten()

# Data generation function
def generate_data_cpd(N, csv1_data, csv2_data, T):
    Data = []
    Labels = []
    samples_per_tau = N // (T - 1)
    for tau in range(1, T):
        for _ in range(samples_per_tau):
            index_vec1 = np.random.choice(csv1_data, tau)
            index_vec2 = np.random.choice(csv2_data, T - tau)
            data_point = np.concatenate((index_vec1, index_vec2))
            Data.append(data_point)
            Labels.append(tau - 1)

    X_vec = np.array(Data)
    y_vec = np.array(Labels)

    return X_vec, y_vec

# Define parameters
T = 7
N_test = 500
num_runs = 1
N_train_values = [2000, 5000]

accuracies = []
Pd_vec = np.zeros((len(N_train_values), num_runs))

for i, N_train in enumerate(N_train_values):
    run_accuracies = []

    for run in range(num_runs):
        # Generate training and test data
        X_train, y_train = generate_data_cpd(N_train, csv1_data, csv2_data, T)
        X_test, y_test = generate_data_cpd(N_test, csv1_data, csv2_data, T)

        # Reshape for CNN/RNN
        X_train = X_train[..., np.newaxis]
        X_test = X_test[..., np.newaxis]
        input_shape = X_train.shape[1:]

        # Calculate class weights for imbalanced data
        class_weights = dict(Counter(y_train))
        total_samples = sum(class_weights.values())
        class_weights = {cls: total_samples / (len(class_weights) * count) for cls, count in class_weights.items()}

        # CNN-RNN Model
        inputs = Input(shape=input_shape)

        # CNN layers
        x = Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(inputs)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)

        x = Conv1D(filters=256, kernel_size=3, activation='relu', padding='same')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)

        # RNN (LSTM) layers
        x = LSTM(128, return_sequences=True)(x)
        x = LSTM(64)(x)

        # Fully connected layers
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.4)(x)
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.4)(x)

        # Output layer
        outputs = Dense(T - 1, activation='softmax')(x)

        # Build model
        model = Model(inputs, outputs)

        # Compile model with Adam optimizer
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])

        # Callbacks for learning rate reduction and early stopping
        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=1e-6)
        early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)

        # Train model
        history = model.fit(X_train, y_train, epochs=100, batch_size=64,
                            callbacks=[reduce_lr, early_stopping],
                            class_weight=class_weights, verbose=2)

        # Evaluate on test data
        y_pred = model.predict(X_test, verbose=0)
        predicted_labels = np.argmax(y_pred, axis=1)
        accuracy = np.mean(predicted_labels == y_test)
        run_accuracies.append(accuracy)
        Pd_vec[i, run] = accuracy

    average_accuracy = np.mean(run_accuracies)
    accuracies.append(average_accuracy)
    print(f'N_train = {N_train}, Average Test accuracy: {average_accuracy:.4f}')

# Plotting the accuracy graph
plt.figure(figsize=(10, 6))
plt.plot(N_train_values, accuracies, marker='o')
plt.title('Average Accuracy vs N_train')
plt.xlabel('N_train')
plt.ylabel('Average Accuracy')
plt.grid(True)
plt.show()